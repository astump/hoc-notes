\documentclass{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{url}
\usepackage{hyperref}
\usepackage{proof}
\usepackage{stmaryrd}
%\usepackage{MnSymbol}
\usepackage{parskip}
\usepackage{fullpage}
\usepackage{mathpartir}
\usepackage{subcaption}
\usepackage{float}


\begin{document}

\include{definitions}

\title{Proposal: a Type Theory for Bend}

\author{Aaron Stump}

\maketitle

\section{What is a type theory?}

A type theory is a statically typed programming language that can be
understood as a logic.  Programs are viewed as proofs, and the types
of programs are viewed as the formulas they prove.  This famous idea
is called the Curry-Howard isomorphism.  A very simple example is the
program $\lam{x}{\lam{y}{x}}$, which takes in input $x$ and then input
$y$, and returns $x$.  This program can be given the type $A \to B \to
A$, for any types $A$ and $B$.  That type expresses that the program
takes in an input of type $A$ and then one of type $B$, and returns a
result of type $A$.  That indeed correctly describes the behavior of
$\lam{x}{\lam{y}{x}}$.  But it is also a valid logical formula, where
the $\to$ operator is implication: $A$ implies $B$ implies $A$ for the
trivial reason that $A$ implies $A$, and adding an extra assumption
that $B$ is true does not change that fact.  To go beyond just
propositional logic, type theories use more expressive types than just
implications.  We will see examples below.

To be interpreted as a logic, it is not enough to have a way
to view the types of a programming language as formulas.  We must
ensure that it is not possible to prove false formulas.  So the
language must be logically sound.  In type theory, an essential part
of ensuring logical soundness is to guarantee that all programs
terminate.  The reason for this is that an infinite loop can be
viewed, in most programming languages, as having any type one wants.
So you can prove the formula \textsf{False} by writing a diverging
program.

Much theoretical effort has been expended on techniques for proving
logical soundness of type theories, by showing that all programs
are guaranteed to terminate.  Bend has a very interesting original
approach to this problem, which we consider next.

\section{Bend's approach to logical soundness}

There are two current traditions for devising type theories, that
should be mentioned for comparison with Bend's approach:

\begin{enumerate}
\item \textbf{Church-style} type theory builds up a notion of typed
  terms (programs), where the types are inherent to those terms.  By a
  difficult argument, one shows that all well-typed programs
  terminate.  So the type system is enforcing termination, in addition
  to other properties usually enforced by static typing.  From
  termination, it is then easy to argue that the system is logically
  sound.  This is because it is relatively easy to show that values,
  which are the final results of computation, cannot have type False.

\item \textbf{Curry-style} type theory starts with a notion of
  type-free program, and then adds types to describe properties of the
  behavior of programs.  For example, the identity function can be
  described as having type $X \to X$ for any type $X$, as it is
  guaranteed to take an input of type $X$ and return an output of type
  $X$ (namely, the input it was given).  A difficult argument is still
  required to show that typing enforces termination.  But the language
  design is made quite a bit easier by not having types be inherent
  parts of programs.  This is because in reasoning about programs,
  one does not then have to reason about types inside them.  Programs
  are type-free, and typing comes second.  In fact, the slogan I propose
  for this style of type theory is ``Computation First'' (because we
  first explain what type-free programs are and how they execute, and
  only afterwards use types to describe their properties).  The great
  computer scientist Jean-Louis Krivine puts it simply: ``types can
  be thought of as properties of $\lambda$-terms''~\cite[page 43]{krivine93}.
\end{enumerate}

Bend's philosophy can be viewed as a strengthened form of Curry-style
type theory, with the modified slogan: ``Terminating Computation
First''.  The idea, proposed by Victor Taelin, is similar to
Curry-style type theory, where one first defines type-free programs,
and how they compute.  But differently, these programs are designed so
that they are guaranteed to terminate, without reference to any notion
of typing.  Just the structure of the programs and the rules for how
the execute are sufficient to establish that all programs terminate.
Giving a detailed proof of that fact is still not trivial, but
expected to be much simpler than the approaches based on typing.  And
then one has a lot of freedom to design a type system on top of the
terminating type-free language.  Now the only requirement is that the
language should have the usual type-safety property that ones expects
of any statically typed programming language. This is vastly easier to
achieve than crafting a type system that enforces termination.

\section{Bend's core language}

The syntax of Bend's core programming language is shown in
Figure~\ref{fig:pl}.  Programs, called terms, can be variables $x$,
machine integers $i$, or infix applications $t\ o\ t'$ of arithmetic
operators $o$ to arguments $t$ and $t'$.  They can also be anonymous
functions $\lam{x}{s}$, with the restriction that $x$ may be used at
most once in $s$.  Some restriction is needed, or else it is very easy
to write diverging $\lambda$-terms.  Traditionally, type theories have
restricted anonymous functions using types.  With Bend's approach to
logical consistency, we need a type-free way to enforce termination of
$\lambda$-terms.  One method is to restrict how often a variable may
be used in an anonymous function.  Bend requires $\lambda$-bound
variables to be used at most once.  It is well known that this
restriction ensures termination.  It does impose serious limits on
programs written with anonymous functions, but we will see that the
way recursion works expands the possibilities greatly (while
preserving termination).

Returning to the syntax: we have applications $t\ t'$ of a term $t$
being used as a function to term $t'$ given as the argument to that
function.  We have a trivial piece of data $\wunit$, that is useful as
a placehold.  We could use a machine integer $i$ as a base case
instead, but we will see that with typing, it is more convenient to
have a separate trivial piece of data.  That is $\wunit$.  We have a
way to form structured data $\ctor{l}{n}{r}$, and a term $\wrec{r}{t}$
for recursing over such data.  These constructs constitue a version of
what is known as W-types, and they will be presented in detail below.

\begin{figure}
  \[
  \begin{array}{llll}
    \textit{Variables}  & x,y,z,\ldots & \ &\ \\
    \textit{Machine integers}  & i,j,\ldots & ::= & 0\ |\ 1\ | \ \cdots \\
    \textit{Arithmetic operators} & o & ::= & +\ |\ *\ |\ \cdots \\        
    \textit{Terms} & s,r,t & ::= & x\ |\ i\ |\ t\ o\ t'\ |\ \lam{x}{s}\ |\ t\ t'\ |\ \wunit\ |\ \ctor{l}{n}{r}\ |\ \wrec{r}{t}
  \end{array}
\]
\caption{The syntax for Bend's programming language.  In $\lambda$-abstractions, the variable $x$ is allowed
  to occur at most once in the body $s$}
\label{fig:pl}
\end{figure}

\section{Structured data}

To implement data structures, every programming language needs some
approach to creating and processing structured data.  In Bend, the
main form of structured data is the construction $\ctor{l}{n}{r}$.
Before we discuss this, though, it is very helpful to have a way of
constructing pairs $(t,t')$.  There are several
possibilities for how to do this, but here we propose to
$\lambda$-encode them.  So we take this definition:
\[
(t,t') := \lam{c}{c\ t\ t'}
\]
\noindent There are two benefits to this choice.  First, we do not
need to add another primitive construction to the language for pairs,
as we are defining pairing.  Second, we gain affine access to the
components of a pair.  That is, given a pair, we write a function
which is given both components at once.  If instead we had primitive
accessors like $p.1$ and $p.2$ for accessing the components of a pair
$p$, we would not be able to write simple functions like the one
that swaps the components of a pair, as an affine function.  For
such a function would be written as
\[
\lam{p}{(p.2,p.1)}
\]
\noindent where we can see that the $\lambda$-bound input variable is used twice.
Instead, we define swapping of pair $p$ as
\[
p\ \lam{x}{\lam{y}{(y,x)}}
\]
\noindent This looks a little peculiar, but recall that pairs are defined to be functions.
So if $p$ is $(1,2)$, for example, then we will have this computational behavior:
\[
(1,2)\ \lam{x}{\lam{y}{(y,x)}} \ =\ (\lam{c}{c\ 1\ 2})\ \lam{x}{\lam{y}{(y,x)}}\ \leadsto\ (\lam{x}{\lam{y}{(y,x)}})\ 1\ 2 \leadsto^* (2,1)
\]
\noindent Note that the $\lambda$-abstractions shown are all linear: the input variables $c$, $x$, and $y$ are used exactly once in the terms
where they are $\lambda$-bound.

\bibliographystyle{plain}
\bibliography{main}

\end{document}
